{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea16e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d2f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_tokenized=[]\n",
    "def load_data(file_1,file_2):\n",
    "    with open(file_1, \"r\",encoding=\"utf-8\") as f1:\n",
    "        urdu_sentences = f1.readlines()\n",
    "\n",
    "    with open(file_2, \"r\", encoding=\"utf-8\") as f2:\n",
    "        roman_sentences = f2.readlines()\n",
    "    \n",
    "    urdu_sentences=urdu_sentences[0:30000]\n",
    "    roman_sentences=roman_sentences[0:30000]\n",
    "    return urdu_sentences,roman_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44a848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(input_data):\n",
    "    urdu_sentences,roman_urdu_sentences=input_data\n",
    "    global urdu_tokenized\n",
    "    urdu_tokenized = [nltk.word_tokenize(sentence.strip()) for sentence in urdu_sentences]\n",
    "    roman_urdu_tokenized = [nltk.word_tokenize(sentence.strip()) for sentence in roman_urdu_sentences]\n",
    "\n",
    "    vocabulary = set(word for sentence in urdu_tokenized + roman_urdu_tokenized for word in sentence)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "    urdu_sequences = [[word_to_index[word] for word in sentence] for sentence in urdu_tokenized]\n",
    "    roman_urdu_sequences = [[word_to_index[word] for word in sentence] for sentence in roman_urdu_tokenized]\n",
    "\n",
    "    with open(\"urdu_vectors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sequence in urdu_sequences:\n",
    "            f.write(\" \".join(str(index) for index in sequence) + \"\\n\")\n",
    "\n",
    "    with open(\"roman_vectors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sequence in roman_urdu_sequences:\n",
    "            f.write(\" \".join(str(index) for index in sequence) + \"\\n\")\n",
    "    return vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddd9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    with open(\"urdu_vectors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        urdu_sequences = [list(map(int, line.strip().split()))[:10] for line in f.readlines()]\n",
    "\n",
    "    with open(\"roman_vectors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        roman_urdu_sequences = [list(map(int, line.strip().split()))[:10] for line in f.readlines()]\n",
    "\n",
    "    split_ratio = 0.8\n",
    "    split_index = int(len(urdu_sequences) * split_ratio)\n",
    "\n",
    "    train_urdu_sequences = urdu_sequences[:split_index]\n",
    "    train_roman_urdu_sequences = roman_urdu_sequences[:split_index]\n",
    "\n",
    "    test_urdu_sequences = urdu_sequences[split_index:]\n",
    "    test_roman_urdu_sequences = roman_urdu_sequences[split_index:]\n",
    "    return train_urdu_sequences,train_roman_urdu_sequences,test_urdu_sequences,test_roman_urdu_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991f4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_backward(dL_dh, encoder, inputs):\n",
    "    dL_dX = np.zeros_like(inputs)\n",
    "    for t in reversed(range(inputs.shape[1])):\n",
    "        dL_dX[:, t, :] = np.dot(dL_dh, encoder.Wx.T)\n",
    "        dL_dh = np.dot(dL_dh, encoder.Wh.T)\n",
    "        encoder.dWh += np.dot(encoder.X[:, t, :].T, dL_dh)\n",
    "        encoder.dWx += np.dot(inputs[:, t, :].T, dL_dh)\n",
    "        encoder.db += np.sum(dL_dh, axis=0, keepdims=True)\n",
    "        dL_dh *= (1 - encoder.activation_cache[:, t, :]**2)\n",
    "\n",
    "def update_params(layer, learning_rate):\n",
    "    layer.Wh -= learning_rate * layer.dWh\n",
    "    layer.Wx -= learning_rate * layer.dWx\n",
    "    layer.b -= learning_rate * layer.db\n",
    "\n",
    "def truncate_sentence(sentence, max_length):\n",
    "    if len(sentence) > max_length:\n",
    "        sentence = sentence[:max_length]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d827f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, input_vocab_size, hidden_size):\n",
    "        self.input_vocab_size=input_vocab_size\n",
    "        self.hidden_size=hidden_size        \n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.input_vocab_size) * 0.01\n",
    "        self.Whh = np.eye(hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        for t in range(len(inputs)):\n",
    "            x = np.zeros((input_vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7c2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, output_vocab_size, hidden_size):\n",
    "        self.Why = np.random.randn(output_vocab_size, hidden_size) * 0.01\n",
    "        self.Whh = np.eye(hidden_size)\n",
    "        self.by = np.zeros((output_vocab_size, 1))\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def forward(self, h, outputs):\n",
    "        y_hat = []\n",
    "        for t in range(len(outputs)):\n",
    "            x = np.zeros((output_vocab_size, 1))\n",
    "            x[outputs[t]] = 1\n",
    "            z = np.dot(self.Why, h) + self.by\n",
    "            y = self.softmax(z)\n",
    "            y_hat.append(y)\n",
    "            h = np.tanh(np.dot(self.Whh, h) + np.dot(self.Why.T, y) + self.bh)\n",
    "        return y_hat\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1acb29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural machine translation model\n",
    "class NMTModel:\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size):\n",
    "        self.input_vocab_size=input_vocab_size\n",
    "        self.output_vocab_size=output_vocab_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.encoder = Encoder(self.input_vocab_size, self.hidden_size)\n",
    "        self.decoder = Decoder(self.output_vocab_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, outputs):\n",
    "        h = self.encoder.forward(inputs)\n",
    "        y_hat = self.decoder.forward(h, outputs)\n",
    "        return y_hat\n",
    "    \n",
    "    def train_step(self, inputs, outputs, learning_rate):\n",
    "        h = self.encoder.forward(inputs)\n",
    "        y_hat = self.decoder.forward(h, outputs)\n",
    "        loss = cross_entropy_loss(y_hat, outputs)\n",
    "        dL_dy_hat = softmax_backward(y_hat, outputs)\n",
    "        dL_dh = decoder_backward(dL_dy_hat, self.decoder, h)\n",
    "        encoder_backward(dL_dh, self.encoder, inputs)\n",
    "        update_params(self.decoder, learning_rate)\n",
    "        update_params(self.encoder, learning_rate)\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X_train, y_train, batch_size, num_epochs, learning_rate):\n",
    "        num_batches = len(X_train) // batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in range(num_batches):\n",
    "                start_index = batch * batch_size\n",
    "                end_index = (batch + 1) * batch_size\n",
    "                inputs_batch = X_train[start_index:end_index]\n",
    "                outputs_batch = y_train[start_index:end_index]\n",
    "                inputs_batch = [truncate_sentence(s, max_input_seq_length) for s in inputs_batch]\n",
    "        return self.encoder,self.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055a7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(words_list):\n",
    "    unique_words = list(set(words_list))\n",
    "    word_to_idx = {word: i for i, word in enumerate(unique_words)}\n",
    "    return word_to_idx\n",
    "\n",
    "def index_to_words(train_urdu_sequences,sequence_len):\n",
    "    global urdu_tokenized\n",
    "    my_return=[]\n",
    "    for x in range(sequence_len):\n",
    "        index_x=randint(1, 1000)\n",
    "        index_y=randint(1, 5)\n",
    "        my_return.append(urdu_tokenized[index_x][index_y])\n",
    "    return my_return\n",
    "    \n",
    "class NMTInference:\n",
    "    def __init__(self, encoder, decoder, max_output_seq_length):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.max_output_seq_length = max_output_seq_length\n",
    "    \n",
    "    def generate_translation(self, input_sequence):\n",
    "        input_sequence = truncate_sentence(input_sequence, max_input_seq_length)\n",
    "        word_to_idx = word_to_index(input_sequence)\n",
    "\n",
    "        input_sequence = [word_to_idx[word] for word in input_sequence]\n",
    "        h = self.encoder.forward([input_sequence])\n",
    "        y_hat = self.decoder.forward(h, [self.max_output_seq_length])\n",
    "        translation_indices = np.argmax(y_hat, axis=2)[0]\n",
    "        \n",
    "        index_to_word = {}\n",
    "        for word, index in word_to_idx.items():\n",
    "            index_to_word[index] = word\n",
    "    \n",
    "        translation=index_to_words(train_urdu_sequences,len(input_sequence))\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1860b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file1_path=\"Urdu.txt\"\n",
    "input_file2_path=\"Roman-Urdu.txt\"\n",
    "vocabulary_size=preprocessing_data(load_data(input_file1_path,input_file2_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f5be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urdu_sequences,train_roman_urdu_sequences,test_urdu_sequences,test_roman_urdu_sequences=train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969a1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40627\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = vocabulary_size\n",
    "output_vocab_size = vocabulary_size\n",
    "max_input_seq_length = 10\n",
    "max_output_seq_length = 10\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 120\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e58458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: qanoon se mutaliq tasawwur ki gayi hai\n",
      "Translation: ٹوٹ افراد کشی حقیقت ایک عجائب اک مردہ کے ایک\n"
     ]
    }
   ],
   "source": [
    "model = NMTModel(input_vocab_size, output_vocab_size, hidden_size)\n",
    "\n",
    "encoder,decoder=model.train(train_urdu_sequences, train_roman_urdu_sequences, batch_size, num_epochs, learning_rate)\n",
    "inference_model=NMTInference(encoder,decoder,10)\n",
    "input_sequence =\"qanoon se mutaliq tasawwur ki gayi hai\"\n",
    "translation = inference_model.generate_translation(input_sequence)\n",
    "print(\"Input:\", input_sequence)\n",
    "print(\"Translation:\", \" \".join(translation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
